{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edcad647-9666-4ef9-bbeb-b6e79d24fe79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# DataSet Analysis Langchain Batch\n",
    "\n",
    "\n",
    "\n",
    "## Details\n",
    "This notebook utilizes Azure OpenAi to produce individual data analysis for specified tables in a dataframe in batch. This demonstration focuses on table residing in the Unity Catalog, though any table converted to the dataframe can be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0732619c-5960-48b0-ba00-f2cd32c9b854",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reinstall Specific Numpy Version"
    }
   },
   "outputs": [],
   "source": [
    "# Force reinstall a specific version of numpy package\n",
    "%pip install --force-reinstall numpy==1.26.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed838fd-4d7f-4cfa-8142-140fcd619c9f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Latest Langchain Package Quietly"
    }
   },
   "outputs": [],
   "source": [
    "# Install the latest version of the langchain package quietly\n",
    "%pip install -qU langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b6b1cec-be99-41cf-9cab-a2cc6e9a6ba0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Updated Langchain-OpenAI Package"
    }
   },
   "outputs": [],
   "source": [
    "# Install the latest version of the langchain-openai package quietly\n",
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17740b2c-6126-40ad-a933-9b796391296a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Latest Langchain-Community Package Quietly"
    }
   },
   "outputs": [],
   "source": [
    "# Install the latest version of the langchain-community package quietly\n",
    "# %pip install -qU langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28e46b77-bb2a-4d2c-ac4d-95435eea8684",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Latest Langchain-Experimental Package Quietly"
    }
   },
   "outputs": [],
   "source": [
    "# Install the latest version of the langchain-experimental package quietly\n",
    "%pip install -qU langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ce9531-c5da-422e-a4f8-039015349f28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Latest Tabulate Package Quietly"
    }
   },
   "outputs": [],
   "source": [
    "# Install the latest version of the tabulate package quietly\n",
    "%pip install -qU tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70042d72-6ecc-4016-957f-b2acf92442ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/serena-ruan/mlflow.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdf02298-59b5-4e6b-b848-62eb37de6340",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Process for Current Notebook Session"
    }
   },
   "outputs": [],
   "source": [
    "# Restart the Python process for the current notebook session\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad675dd-45bb-4075-a80b-a382fc240fbe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialize Libraries and Set Up Logging Configuration"
    }
   },
   "outputs": [],
   "source": [
    "import logging  # For logging information\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import pyspark.sql.functions as F\n",
    "import time  # For time-related functions\n",
    "import warnings  # For handling warnings\n",
    "from datetime import datetime  # For manipulating dates and times\n",
    "from IPython.display import display, Markdown  # For displaying rich content in notebooks\n",
    "from langchain_openai import AzureChatOpenAI  # For Azure OpenAI model integration\n",
    "from langchain.agents import AgentExecutor  # For executing agents\n",
    "from langchain.agents.agent_types import AgentType  # For defining agent types\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent, create_spark_dataframe_agent # For creating pandas dataframe agents\n",
    "from pyspark.sql.types import *  # For defining Spark data types\n",
    "from tabulate import tabulate  # For tabulating data\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)  # Ignore future warnings\n",
    "\n",
    "logger = logging.getLogger(__name__)  # Create a logger\n",
    "\n",
    "logger.propagate = False  # Prevent the logger from propagating messages to the root logger\n",
    "\n",
    "logger.setLevel(logging.INFO)  # Set the logging level to INFO\n",
    "\n",
    "stream_handler = logging.StreamHandler()  # Create a stream handler\n",
    "\n",
    "stream_handler.setLevel(logging.INFO)  # Set the stream handler logging level to INFO\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')  # Define a log message format\n",
    "\n",
    "stream_handler.setFormatter(formatter)  # Set the formatter for the stream handler\n",
    "\n",
    "logger.addHandler(stream_handler)  # Add the stream handler to the logger\n",
    "\n",
    "# Function to insert a value at the top of a markdown string\n",
    "def insert_at_top(markdown_string, value_to_insert):\n",
    "    updated_markdown = f\"{value_to_insert}\\n\\n{markdown_string}\"\n",
    "    return updated_markdown\n",
    "\n",
    "# Function to map PySpark data types to T-SQL data types\n",
    "def get_tsql_dtype(data_type):\n",
    "    # Dictionary mapping PySpark data types to T-SQL data types\n",
    "    pyspark_to_tsql = {\n",
    "        \"string\": \"varchar\",\n",
    "        \"int\": \"int\",\n",
    "        \"bigint\": \"bigint\",\n",
    "        \"long\": \"bigint\",\n",
    "        \"float\": \"float\",\n",
    "        \"double\": \"float\",\n",
    "        \"boolean\": \"bit\",\n",
    "        \"date\": \"date\",\n",
    "        \"timestamp\": \"datetime\",\n",
    "        \"binary\": \"varbinary\",\n",
    "        \"decimal\": \"decimal\"\n",
    "    }\n",
    "    # Retrieve the corresponding T-SQL data type for the given PySpark data type\n",
    "    field_type = pyspark_to_tsql[data_type]\n",
    "    return field_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c32568f8-bc35-4860-a5bb-b1f9a768b947",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Up API Keys and Initialize Azure OpenAI Model"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the OpenAI API key from Databricks secrets\n",
    "api_key = dbutils.secrets.get(scope=\"djsdbsecrets\", key=\"openai-key\")\n",
    "\n",
    "# Define the Azure OpenAI endpoint\n",
    "azure_endpoint = dbutils.secrets.get(scope=\"djsdbsecrets\", key=\"openai-uri\")\n",
    "\n",
    "# Define the deployment name for the OpenAI model\n",
    "deployment = \"gpt-4o\"\n",
    "\n",
    "# Define the catalog name for the database\n",
    "catalog = \"generaldata\"\n",
    "\n",
    "# Define the schema name for the database\n",
    "schema = 'dataanalysis'\n",
    "\n",
    "# Initialize the AzureChatOpenAI model with the specified parameters\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=deployment, \n",
    "    azure_endpoint=azure_endpoint,  \n",
    "    openai_api_version=\"2024-12-01-preview\",  \n",
    "    openai_api_key=api_key,  \n",
    "    model=deployment,\n",
    "    temperature=0 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe15d21-519c-4bbf-8c44-3a881789a037",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process Tables and Generate Analysis Reports"
    }
   },
   "outputs": [],
   "source": [
    "table = \"air_quality\"\n",
    "\n",
    "# Construct the full table name using catalog, schema, and table name\n",
    "full_table_name = f\"{catalog}.{schema}.{table}\"\n",
    "\n",
    "# Log the start of processing for the current table\n",
    "logger.info(f\"Processing table {full_table_name}\")\n",
    "\n",
    "# Generate a timestamp string for the current date and time in the format \"YYYY-MM-DD_HH-MM-SS\"\n",
    "file_date_stamp = str(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "\n",
    "# Define the analysis report name based on the full table name\n",
    "analysis_report_name = f\"analysis_output_{file_date_stamp}_{full_table_name.replace('.','_')}.md\"\n",
    "\n",
    "# Create a SQL query to select the first 20 rows from the current table\n",
    "table_select_query = f\"SELECT * FROM {full_table_name}\"\n",
    "\n",
    "# Execute the query to retrieve data from the current table\n",
    "analysis_df = spark.sql(table_select_query)\n",
    "\n",
    "# Initialize a list to store columns with binary data types\n",
    "columns_to_drop = []\n",
    "\n",
    "# Iterate over the data types of the DataFrame columns\n",
    "for dt in analysis_df.dtypes:\n",
    "# Check if the column data type is binary\n",
    "    if dt[1] == 'binary':\n",
    "        # Add the column name to the list of columns to drop\n",
    "        columns_to_drop.append(dt[0])\n",
    "        # Log the column name that will be dropped\n",
    "        logger.info(f\"The following column(s) will be dropped from analysis dataframe as they are binary data types: {dt[0]}\")\n",
    "\n",
    "# Drop the columns with binary data types from the DataFrame\n",
    "analysis_df = analysis_df.drop(*columns_to_drop) \n",
    "\n",
    "# # Convert Spark DataFrame to Pandas DataFrame\n",
    "# open_ai_df = analysis_df.toPandas()\n",
    "\n",
    "# Pause execution for 30 seconds to avoid hitting rate limits or overloading the API\n",
    "time.sleep(60)\n",
    "\n",
    "# Create a Spark DataFrame agent using the Azure OpenAI model\n",
    "agent = create_spark_dataframe_agent(\n",
    "llm,  # Azure OpenAI model initialized in the previous cell\n",
    "analysis_df,  # DataFrame to be analyzed\n",
    "agent_type=AgentType.OPENAI_FUNCTIONS,  # Specify the agent type\n",
    "agent_executor_kwargs={\"handle_parsing_errors\": True},  # Handle parsing errors\n",
    "allow_dangerous_code=True  # Allow execution of potentially dangerous code\n",
    ")\n",
    "\n",
    "\n",
    "# Define the input prompt for the EDA agent\n",
    "eda_input = \"\"\"\n",
    "You are a data analyst given the provided dataframe. Perform and display an Exploratory Data Analysis on the provided dataframe. Please focus on providing a thorough analysis of the dataset. Ensure your analysis is detailed, tailored, and user-centric to help the user understand the data. ALWAYS FINISH THE OUTPUT. Never send partial responses. STRICTLY use the context provided to generate your response. DO NOT GENERATE ANYTHING FROM YOUR PRE-TRAINED MEMORY.  \n",
    "\"\"\"\n",
    "\n",
    "# Invoke the agent with the EDA input to generate the analysis response\n",
    "response = agent.invoke({\"input\": eda_input.strip()})\n",
    "\n",
    "# Format the output as Markdown\n",
    "output = Markdown(response['output']) \n",
    "\n",
    "# Create the header for the analysis report including the data types table\n",
    "output_header = f\"# Data Analysis for table {full_table_name}\"\n",
    "\n",
    "# Insert the header at the top of the analysis output\n",
    "\n",
    "# vol_path = \"/Volumes/generaldata/dataanalysis/markdowndocs\"\n",
    "\n",
    "# with open(f\"{vol_path}/{analysis_report_name}\", \"w\") as file:\n",
    "#     file.write(str(updated_output))\n",
    "\n",
    "# logging.info(f\"Analysis report saved to {vol_path}/{analysis_report_name}\")\n",
    "display(Markdown(insert_at_top(output.data,output_header)))  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6450159100984109,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DataSet_Analysis_Langchain_Batch",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
